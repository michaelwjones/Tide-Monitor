# W&B Sweep configuration for hyperparameter tuning
program: train_wandb.py
method: bayes
metric:
  name: val_loss
  goal: minimize

parameters:
  # Training parameters
  batch_size:
    values: [2, 4, 8]
  learning_rate:
    min: 1e-5
    max: 1e-3
    distribution: log_uniform
  weight_decay:
    min: 1e-6
    max: 1e-4
    distribution: log_uniform
  num_epochs:
    value: 50
  
  # Model architecture parameters
  d_model:
    values: [128, 256, 512]
  nhead:
    values: [4, 8, 16]
  num_encoder_layers:
    values: [3, 6, 9]
  num_decoder_layers:
    values: [2, 3, 4]
  dim_feedforward:
    values: [512, 1024, 2048]
  dropout:
    min: 0.1
    max: 0.3
    distribution: uniform
  
  # Learning rate scheduler
  scheduler_t0:
    values: [5, 10, 15]
  
  # Data augmentation
  missing_prob:
    min: 0.01
    max: 0.05
    distribution: uniform
  gap_prob:
    min: 0.02
    max: 0.08
    distribution: uniform
  noise_std:
    min: 0.05
    max: 0.2
    distribution: uniform